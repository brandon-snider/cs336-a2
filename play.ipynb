{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assumption: batch size = 128, sequence length = 1024\n",
      "-\n",
      "Total parameters: 219848638464\n",
      "Parameter bytes: 819.0 GB\n",
      "Accumulated gradient bytes: 819.0 GB\n",
      "Optimizer bytes: 1638.0 GB\n",
      "Static memory: 3276.0 GB\n",
      "-\n",
      "Activation bytes: 2142.0 GB\n",
      "Number of samples (B * L): 131072\n",
      "Activation bytes per sample: 0.0163421630859375 GB\n",
      "-\n",
      "Number of H100 GPUs: 68\n"
     ]
    }
   ],
   "source": [
    "# Communication Accounting, part a)\n",
    "# Assumptions:\n",
    "# - Each FFN is two linear layers â€” one from d_model to d_ff and one from d_ff to d_model (no activation)\n",
    "# - Each block consists entirely of a single FFN\n",
    "# - We omit attention, input embeddings, norms, and the output linear layer (i.e. we only count the FFN parameters)\n",
    "# - We are not using activation checkpointing, so must keep the input to each linear layer\n",
    "\n",
    "import math\n",
    "\n",
    "print(\"Assumption: batch size = 128, sequence length = 1024\\n-\")\n",
    "\n",
    "def bytes_to_mb(bytes):\n",
    "    return bytes / (1024 * 1024)\n",
    "\n",
    "def bytes_to_gb(bytes):\n",
    "    return bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "d_model = 16384\n",
    "d_ff = 53248\n",
    "num_blocks = 126\n",
    "\n",
    "params_per_block = d_model * d_ff + d_ff * d_model\n",
    "total_params = params_per_block * num_blocks\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "param_bytes = total_params * 4 # keeping params, accumulated gradients, and optimizer state in float32\n",
    "accum_grad_bytes = param_bytes # one gradient per parameter, in float32\n",
    "optim_bytes = param_bytes * 2 # first and second moment for AdamW, in float32\n",
    "m_static = param_bytes + accum_grad_bytes + optim_bytes\n",
    "\n",
    "print(f\"Parameter bytes: {bytes_to_gb(param_bytes)} GB\")\n",
    "print(f\"Accumulated gradient bytes: {bytes_to_gb(accum_grad_bytes)} GB\")\n",
    "print(f\"Optimizer bytes: {bytes_to_gb(optim_bytes)} GB\")\n",
    "print(f\"Static memory: {bytes_to_gb(m_static)} GB\")\n",
    "print(\"-\")\n",
    "\n",
    "# ----- Activation memory -----\n",
    "\n",
    "# To compute gradients, we need the input to the second linear layer (d_ff) and the first linear layer (d_model)\n",
    "batch_size = 128\n",
    "seq_len = 1024\n",
    "num_samples = batch_size * seq_len\n",
    "elements_per_sample = num_blocks * (d_model + d_ff) \n",
    "activation_bytes_per_sample = elements_per_sample * 2 # we keep activations in bfloat16\n",
    "activation_bytes = activation_bytes_per_sample * num_samples\n",
    "m_act = activation_bytes\n",
    "\n",
    "print(f\"Activation bytes: {bytes_to_gb(activation_bytes)} GB\")\n",
    "print(f\"Number of samples (B * L): {num_samples}\")\n",
    "print(f\"Activation bytes per sample: {bytes_to_gb(activation_bytes_per_sample)} GB\")\n",
    "\n",
    "print(\"-\")\n",
    "\n",
    "bytes_per_h100 = 80 * 1024**3\n",
    "n_h100 = math.ceil((m_static + m_act) / bytes_per_h100)\n",
    "\n",
    "print(f\"Number of H100 GPUs: {n_h100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of v5p TPUs: 46\n"
     ]
    }
   ],
   "source": [
    "# Communication Accounting, part b)\n",
    "# - Assume your master weights, optimizer state, and half of the activations (in practice every second layer) are sharded across n_fsdp devices\n",
    "# - Write an expression for how much memory this would take per device\n",
    "# - What value does n_fsdp need to be to keep the memory cost under 1 v5p TPU (95GB per device)?\n",
    "# - Deliverable: calculations and a one-sentence response\n",
    "\n",
    "m_act_half = m_act / 2\n",
    "# m_fsdp = (m_static + m_act_half) / n_fsdp\n",
    "m_total = m_static + m_act_half\n",
    "\n",
    "bytes_per_v5p = 95 * 1024**3\n",
    "n_v5p = math.ceil(m_total / bytes_per_v5p)\n",
    "\n",
    "print(f\"Number of v5p TPUs: {n_v5p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
